<document xmlns="http://cnx.rice.edu/cnxml" xmlns:md="http://cnx.rice.edu/mdml">
  <title>Introduction to Algorithms</title>
  <metadata>
  <md:content-id>m29436</md:content-id><md:title>Introduction to Algorithms</md:title>
  <md:abstract/>
  <md:uuid>2f3dbf30-8390-494d-be9e-c6cef8778cc0</md:uuid>
</metadata>

<content>
    <section id="id-090923008323">
      <title>1. Introduction to Algorithms</title>
      <section id="id-0301186528806">
        <title>1.1. Problem solution</title>
        <para id="id4699501">(From Wikipedia, the free encyclopedia)</para>
        <para id="id4699511">Algorithms are essential to the way <link url="http://en.wikipedia.org/wiki/Computer">computers</link> process information, because a <link url="http://en.wikipedia.org/wiki/Computer_program">computer program</link> is essentially an algorithm that tells the computer what specific steps to perform (in what specific order) in order to carry out a specified task, such as calculating employees’ paychecks or printing students’ report cards. Thus, an algorithm can be considered to be any sequence of operations that can be performed by a <link url="http://en.wikipedia.org/wiki/Turing_completeness">Turing-complete</link> system. Authors who assert this thesis include Savage (1987) and Gurevich (2000):</para>
        <para id="id4699562">"...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine" (Gurevich 2000:1) ...according to Savage [1987], "an algorithm is a computational process defined by a Turing machine."(Gurevich 2000:3)</para>
        <para id="id4699571">Typically, when an algorithm is associated with processing information, data are read from an input source or device, written to an output sink or device, and/or stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in a <link url="http://en.wikipedia.org/wiki/Data_structure">data structure</link>, but an algorithm requires the internal data only for specific operation sets called <link url="http://en.wikipedia.org/wiki/Abstract_data_type">abstract data types</link>.</para>
        <para id="id4699605">For any such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).</para>
        <para id="id4699614">Because an algorithm is a precise list of precise steps, the order of computation will almost always be critical to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting 'from the top' and going 'down to the bottom', an idea that is described more formally by <link url="http://en.wikipedia.org/wiki/Control_flow">flow of control</link>.</para>
        <para id="id4699635">Algorithms can be expressed in many kinds of notation, including <link url="http://en.wikipedia.org/wiki/Natural_language">natural languages</link>, <link url="http://en.wikipedia.org/wiki/Pseudocode">pseudocode</link>, <link url="http://en.wikipedia.org/wiki/Flowchart">flowcharts</link>, and <link url="http://en.wikipedia.org/wiki/Programming_language">programming languages</link>. Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode and flowcharts are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements, while remaining independent of a particular implementation language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a <link url="http://en.wikipedia.org/wiki/Computer">computer</link>, but are often used as a way to define or document algorithms.</para>
        <para id="id4699705">There is a wide variety of representations possible and one can express a given <link url="http://en.wikipedia.org/wiki/Turing_machine">Turing machine</link> program as a sequence of machine tables (see more at <link url="http://en.wikipedia.org/wiki/Finite_state_machine">finite state machine</link> and <link url="http://en.wikipedia.org/wiki/State_transition_table">state transition table</link>), as flowcharts (see more at <link url="http://en.wikipedia.org/wiki/State_diagram">state diagram</link>), or as a form of rudimentary <link url="http://en.wikipedia.org/wiki/Machine_code">machine code</link> or <link url="http://en.wikipedia.org/wiki/Assembly_code">assembly code</link> called "sets of quadruples" (see more at <link url="http://en.wikipedia.org/wiki/Turing_machine">Turing machine</link>).</para>
        <para id="id4699794">Sometimes it is helpful in the description of an algorithm to supplement small "flow charts" (state diagrams) with natural-language and/or arithmetic expressions written inside "<link url="http://en.wikipedia.org/wiki/Block_diagram">block diagrams</link>" to summarize what the "flow charts" are accomplishing.</para>
        <para id="id4699814">Representations of algorithms are generally classed into three accepted levels of Turing machine description (Sipser 2006:157):</para>
        <list id="id4699821" list-type="bulleted">
          <item>1 High-level description:</item>
        </list>
        <para id="id4699836">"...prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head"</para>
        <list id="id4699843" list-type="bulleted">
          <item>2 Implementation description:</item>
        </list>
        <para id="id4699859">"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level we do not give details of states or transition function"</para>
        <list id="id4699866" list-type="bulleted">
          <item>3 Formal description:</item>
        </list>
        <para id="id4699882">Most detailed, "lowest level", gives the Turing machine's "state table".</para>
        <para id="id4699888">As it happens, it is important to know how much of a particular resource (such as time or storage) is required for a given algorithm. Methods have been developed for the <link url="http://en.wikipedia.org/wiki/Analysis_of_algorithms">analysis of algorithms</link> to obtain such quantitative answers; for example, the algorithm above has a time requirement of O(n), using the <link url="http://en.wikipedia.org/wiki/Big_O_notation">big O notation</link> with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore it is said to have a space requirement of O(1). (Note that the size of the inputs is not counted as space used by the algorithm.)</para>
        <para id="id4227363">Different algorithms may complete the same task with a different set of instructions in less or more time, space, or effort than others. For example, given two different recipes for making potato salad, one may have peel the potato before boil the potato while the other presents the steps in the reverse order, yet they both call for these steps to be repeated for all potatoes and end when the potato salad is ready to be eaten.</para>
        <para id="id4227385">The <link url="http://en.wikipedia.org/wiki/Analysis_of_algorithms">analysis and study of algorithms</link> is a discipline of <link url="http://en.wikipedia.org/wiki/Computer_science">computer science</link>, and is often practiced abstractly without the use of a specific <link url="http://en.wikipedia.org/wiki/Programming_language">programming language</link> or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation.</para>
        <para id="id4227431">There are various ways to classify algorithms, each with its own merits.</para>
        <section id="id-890957273029">
          <title>Classification by implementation</title>
          <para id="id4227449">One way to classify algorithms is by implementation means.</para>
          <list id="id4227454" list-type="bulleted">
            <item>Recursion or iteration: A <link url="http://en.wikipedia.org/wiki/Recursive_algorithm">recursive algorithm</link> is one that invokes (makes reference to) itself repeatedly until a certain condition matches, which is a method common to <link url="http://en.wikipedia.org/wiki/Functional_programming">functional programming</link>. <link url="http://en.wikipedia.org/wiki/Iteration">Iterative</link> algorithms use repetitive constructs like <link url="http://en.wikipedia.org/wiki/Control_flow#Loops">loops</link> and sometimes additional data structures like <link url="http://en.wikipedia.org/wiki/Stack_%28data_structure%29">stacks</link> to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, <link url="http://en.wikipedia.org/wiki/Towers_of_hanoi">towers of hanoi</link> is well understood in recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.</item>
          </list>
          <list id="id4227577" list-type="bulleted">
            <item>Logical: An algorithm may be viewed as controlled <link url="http://en.wikipedia.org/wiki/Deductive_reasoning">logical deduction</link>. This notion may be expressed as:</item>
          </list>
          <para id="id4227609">Algorithm = logic + control. </para>
          <para id="id4227620">The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the <link url="http://en.wikipedia.org/wiki/Logic_programming">logic programming</link> paradigm. In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant <link url="http://en.wikipedia.org/wiki/Formal_semantics_of_programming_languages">semantics</link>: a change in the axioms has a well defined change in the algorithm.</para>
          <list id="id4227666" list-type="bulleted">
            <item>Serial or parallel or distributed: Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to <link url="http://en.wikipedia.org/wiki/Parallel_algorithm">parallel algorithms</link> or <link url="http://en.wikipedia.org/wiki/Distributed_algorithms">distributed algorithms</link>. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilise multiple machines connected with a <link url="http://en.wikipedia.org/wiki/Computer_Network">network</link>. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms, and are called inherently serial problems.</item>
          </list>
          <list id="id3973416" list-type="bulleted">
            <item>Deterministic or non-deterministic: <link url="http://en.wikipedia.org/wiki/Deterministic_algorithm">Deterministic algorithms</link> solve the problem with exact decision at every step of the algorithm whereas <link url="http://en.wikipedia.org/wiki/Non-deterministic_algorithm">non-deterministic algorithm</link> solve problems via guessing although typical guesses are made more accurate through the use of <link url="http://en.wikipedia.org/wiki/Heuristics">heuristics</link>.</item>
          </list>
          <list id="id3973488" list-type="bulleted">
            <item>Exact or approximate: While many algorithms reach an exact solution, <link url="http://en.wikipedia.org/wiki/Approximation_algorithm">approximation algorithms</link> seek an approximation that is close to the true solution. Approximation may use either a deterministic or a random strategy. Such algorithms have practical value for many hard problems.</item>
          </list>
        </section>
        <section id="id-892971142947">
          <title>Classification by design paradigm</title>
          <para id="id3973548">Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories will include many different types of algorithms. Some commonly found paradigms include:</para>
          <list id="id3973557" list-type="bulleted">
            <item>Divide and conquer. A <link url="http://en.wikipedia.org/wiki/Divide_and_conquer_algorithm">divide and conquer algorithm</link> repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually <link url="http://en.wikipedia.org/wiki/Recursion">recursively</link>), until the instances are small enough to solve easily. One such example of divide and conquer is <link url="http://en.wikipedia.org/wiki/Mergesort">merge sorting</link>. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in conquer phase by merging them. A simpler variant of divide and conquer is called decrease and conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so conquer stage will be more complex than decrease and conquer algorithms. An example of decrease and conquer algorithm is <link url="http://en.wikipedia.org/wiki/Binary_search_algorithm">binary search algorithm</link>.</item>
            <item><link url="http://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</link>. When a problem shows <link url="http://en.wikipedia.org/wiki/Optimal_substructure">optimal substructure</link>, meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems, and <link url="http://en.wikipedia.org/wiki/Overlapping_subproblems">overlapping subproblems</link>, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, the shortest path to a goal from a vertex in a weighted <link url="http://en.wikipedia.org/wiki/Graph_%28mathematics%29">graph</link> can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and <link url="http://en.wikipedia.org/wiki/Memoization">memoization</link> go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a <link url="http://en.wikipedia.org/wiki/Mathematical_table">table</link> of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.</item>
            <item>The greedy method. A <link url="http://en.wikipedia.org/wiki/Greedy_algorithm">greedy algorithm</link> is similar to a <link url="http://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming algorithm</link>, but the difference is that solutions to the subproblems do not have to be known at each stage; instead a "greedy" choice can be made of what looks best for the moment. The greedy method extends the solution with the best possible decision (not all feasible decisions) at an algorithmic stage based on the current local optimum and the best decision (not all possible decisions) made in previous stage. It is not exhaustive, and does not give accurate answer to many problems. But when it works, it will be the fastest method. The most popular greedy algorithm is finding the minimal spanning tree as given by <link url="http://en.wikipedia.org/wiki/Kruskal%27s_algorithm">Kruskal</link>.</item>
            <item>Linear programming. When solving a problem using <link url="http://en.wikipedia.org/wiki/Linear_programming">linear programming</link>, specific <link url="http://en.wikipedia.org/wiki/Inequality">inequalities</link> involving the inputs are found and then an attempt is made to maximize (or minimize) some linear function of the inputs. Many problems (such as the <link url="http://en.wikipedia.org/wiki/Maximum_flow_problem">maximum flow</link> for directed <link url="http://en.wikipedia.org/wiki/Graph_%28mathematics%29">graphs</link>) can be stated in a linear programming way, and then be solved by a 'generic' algorithm such as the <link url="http://en.wikipedia.org/wiki/Simplex_algorithm">simplex algorithm</link>. A more complex variant of linear programming is called integer programming, where the solution space is restricted to the <link url="http://en.wikipedia.org/wiki/Integers">integers</link>.</item>
            <item><link url="http://en.wikipedia.org/wiki/Reduction_%28complexity%29">Reduction</link>. This technique involves solving a difficult problem by transforming it into a better known problem for which we have (hopefully) <link url="http://en.wikipedia.org/wiki/Asymptotically_optimal">asymptotically optimal</link> algorithms. The goal is to find a reducing algorithm whose <link url="http://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</link> is not dominated by the resulting reduced algorithm's. For example, one <link url="http://en.wikipedia.org/wiki/Selection_algorithm">selection algorithm</link> for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquers.</item>
            <item>Search and enumeration. Many problems (such as playing <link url="http://en.wikipedia.org/wiki/Chess">chess</link>) can be modeled as problems on <link url="http://en.wikipedia.org/wiki/Graph_theory">graphs</link>. A <link url="http://en.wikipedia.org/wiki/Graph_exploration_algorithm">graph exploration algorithm</link> specifies rules for moving around a graph and is useful for such problems. This category also includes <link url="http://en.wikipedia.org/wiki/Search_algorithm">search algorithms</link>, <link url="http://en.wikipedia.org/wiki/Branch_and_bound">branch and bound</link> enumeration and <link url="http://en.wikipedia.org/wiki/Backtracking">backtracking</link>.</item>
            <item>The probabilistic and heuristic paradigm. Algorithms belonging to this class fit the definition of an algorithm more loosely.</item>
          </list>
          <list id="id4545646" list-type="enumerated">
            <item><link url="http://en.wikipedia.org/wiki/Probabilistic_algorithm">Probabilistic algorithms</link> are those that make some choices randomly (or pseudo-randomly); for some problems, it can in fact be proven that the fastest solutions must involve some <link url="http://en.wikipedia.org/wiki/Randomness">randomness</link>.</item>
            <item><link url="http://en.wikipedia.org/wiki/Genetic_algorithm">Genetic algorithms</link> attempt to find solutions to problems by mimicking biological <link url="http://en.wikipedia.org/wiki/Evolution">evolutionary</link> processes, with a cycle of random mutations yielding successive generations of "solutions". Thus, they emulate reproduction and "survival of the fittest". In <link url="http://en.wikipedia.org/wiki/Genetic_programming">genetic programming</link>, this approach is extended to algorithms, by regarding the algorithm itself as a "solution" to a problem.</item>
            <item><link url="http://en.wikipedia.org/wiki/Heuristic_%28computer_science%29">Heuristic</link> algorithms, whose general purpose is not to find an optimal solution, but an approximate solution where the time or resources are limited. They are not practical to find perfect solutions. An example of this would be <link url="http://en.wikipedia.org/wiki/Local_search_%28optimization%29">local search</link>, <link url="http://en.wikipedia.org/wiki/Tabu_search">tabu search</link>, or <link url="http://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</link> algorithms, a class of heuristic probabilistic algorithms that vary the solution of a problem by a random amount. The name "simulated <link url="http://en.wikipedia.org/wiki/Annealing">annealing</link>" alludes to the metallurgic term meaning the heating and cooling of metal to achieve freedom from defects. The purpose of the random variance is to find close to globally optimal solutions rather than simply locally optimal ones, the idea being that the random element will be decreased as the algorithm settles down to a solution.</item>
          </list>
        </section>
        <section id="id-41355148426">
          <title>Classification by field of study</title>
          <para id="id5063917">Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are <link url="http://en.wikipedia.org/wiki/Search_algorithm">search algorithms</link>, <link url="http://en.wikipedia.org/wiki/Sorting_algorithm">sorting algorithms</link>, <link url="http://en.wikipedia.org/wiki/Merge_algorithm">merge algorithms</link>, <link url="http://en.wikipedia.org/wiki/Numerical_analysis">numerical algorithms</link>, <link url="http://en.wikipedia.org/wiki/Graph_theory">graph algorithms</link>, <link url="http://en.wikipedia.org/wiki/String_algorithms">string algorithms</link>, <link url="http://en.wikipedia.org/wiki/Computational_geometry">computational geometric algorithms</link>, <link url="http://en.wikipedia.org/wiki/Combinatorial">combinatorial algorithms</link>, <link url="http://en.wikipedia.org/wiki/Machine_learning">machine learning</link>, <link url="http://en.wikipedia.org/wiki/Cryptography">cryptography</link>, <link url="http://en.wikipedia.org/wiki/Data_compression">data compression</link> algorithms and <link url="http://en.wikipedia.org/wiki/Parsing">parsing techniques</link>.</para>
          <para id="id5064065">Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was originally invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.</para>
        </section>
        <section id="id-737370790174">
          <title>Classification by complexity</title>
          <para id="id5064091">Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.</para>
        </section>
      </section>
      <section id="id-0595500584397">
        <title>1.2. Data models</title>
        <para id="id5064116">(From Wikipedia, the free encyclopedia)</para>
        <para id="id5064127">A data model is an abstract model that describes how <link url="http://en.wikipedia.org/wiki/Data_%28computing%29">data</link> is represented and used.</para>
        <para id="id3702618">The term data model has two generally accepted meanings:</para>
        <list id="id3702629" list-type="enumerated">
          <item>A data model theory i.e. a formal description of how data may be structured and used. See also <link url="http://en.wikipedia.org/wiki/Database_model">database model</link></item>
          <item>A data model instance i.e. applying a data model theory to create a practical data model instance for some particular application. See <link url="http://en.wikipedia.org/wiki/Data_modeling">data modeling</link>.</item>
        </list>
        <para id="id3702711">Data Model Theory</para>
        <para id="id3702719">A data model theory has three main components:</para>
        <list id="id3702729" list-type="bulleted">
          <item>The structural part: a collection of data structures which are used to create databases representing the entities or objects modeled by the database.</item>
          <item>The integrity part: a collection of rules governing the constraints placed on these data structures to ensure structural integrity.</item>
          <item>The manipulation part: a collection of operators which can be applied to the data structures, to update and query the data contained in the database.</item>
        </list>
        <para id="id3702790">For example, in the <link url="http://en.wikipedia.org/wiki/Relational_model">relational model</link>, the structural part is based on a modified concept of the <link url="http://en.wikipedia.org/wiki/Relation_%28mathematics%29">mathematical relation</link>; the integrity part is expressed in <link url="http://en.wikipedia.org/wiki/First-order_logic">first-order logic</link> and the manipulation part is expressed using the <link url="http://en.wikipedia.org/wiki/Relational_algebra">relational algebra</link>, <link url="http://en.wikipedia.org/wiki/Tuple_calculus">tuple calculus</link> and <link url="http://en.wikipedia.org/wiki/Domain_calculus">domain calculus</link>.</para>
        <para id="id3702868">Data Model Instance</para>
        <para id="id3702883"><link url="http://en.wikipedia.org/wiki/Data_modeling">Data modeling</link> is the process of creating a data model instance by applying a data model theory. This is typically done to solve some business enterprise requirement.</para>
        <para id="id3702911">Business requirements are normally captured by a semantic <link url="http://en.wikipedia.org/wiki/Logical_data_model">logical data model</link>. This is transformed into a physical data model instance from which is generated a physical <link url="http://en.wikipedia.org/wiki/Database">database</link>. For more information on the tools and techniques of data modeling, see <link url="http://en.wikipedia.org/wiki/Data_modeling">data modeling</link>.</para>
        <para id="id3702960">For example, a <link url="http://en.wikipedia.org/wiki/Data_modeling">data modeler</link> may use a data modeling tool to create an <link url="http://en.wikipedia.org/wiki/Entity-relationship_model">ERD</link> of the <link url="http://en.wikipedia.org/wiki/Corporate_data_repository">Corporate data repository</link> of some business enterprise. This model is transformed into a <link url="http://en.wikipedia.org/wiki/Relational_model">relational model</link>, which in turn generates a <link url="http://en.wikipedia.org/wiki/Relational_database">relational database</link>.</para>
        <para id="id4710629">
          <figure id="id4710639">
            <media id="id18186137" alt=""><image src="../../media/Zachman Framework Perspectives of Data Focus.jpg" mime-type="image/jpeg" height="294" width="312"/></media>
          </figure>
        </para>
        <para id="id4710663">Pic.1 Zachman Framework Perspectives of Data Focus</para>
        <para id="id4710668">A data model instance may be one of three kinds (according to ANSI in 1975):</para>
        <list id="id4710679" list-type="bulleted">
          <item>a <link url="http://en.wikipedia.org/wiki/Conceptual_schema">conceptual schema</link> (data model) describes the semantics of an organization. This consists of entity classes (representing things of significance to the organization) and relationships (assertions about associations between pairs of entity classes).</item>
          <item>a <link url="http://en.wikipedia.org/wiki/Logical_schema">logical schema</link> (data model) describes the semantics, as represented by a particular data manipulation technology. This consists of descriptions of tables and columns, object oriented classes, and XML tags, among other things.</item>
          <item>a <link url="http://en.wikipedia.org/wiki/Physical_schema">physical schema</link> (data model) describes the physical means by which data are stored. This is concerned with partitions, CPUs, tablespaces, and the like.</item>
        </list>
        <para id="id4710762">The significance of this approach, according to ANSI, is that it allows the three perspectives to be relatively independent of each other. Storage technology can change without affecting either the logical or the conceptual model. The table/column structure can change without (necessarily) affecting the conceptual model. In each case, of course, the structures must remain consistent with the other model. The table/column structure may be different from a direct translation of the entity classes and attributes, but it must ultimately carry out the objectives of the conceptual entity class structure. Early phases of many software development projects emphasize the design of a <link url="http://en.wikipedia.org/wiki/Conceptual_schema">conceptual data model</link>. Such a design can be detailed into a <link url="http://en.wikipedia.org/wiki/Logical_data_model">logical data model</link>. In later stages, this model may be translated into <link url="http://en.wikipedia.org/wiki/Physical_data_model">physical data model</link>.</para>
        <para id="id4710814">In an alternative framework, called the <link url="http://en.wikipedia.org/wiki/Zachman_framework">Zachman Framework</link>, a data model instance may be one of six kinds (according to <link url="http://en.wikipedia.org/wiki/John_Zachman">John Zachman</link>, 1987):</para>
        <list id="id4710850" list-type="bulleted">
          <item>a <link url="http://en.wikipedia.org/wiki/Conceptual_data_model">conceptual data model</link> (schema) consists of entity classes (representing things of significance to the organization).</item>
          <item>a <link url="http://en.wikipedia.org/w/index.php?title=Contextual_data_model&amp;action=edit">contextual data model</link> (schema) describes the semantics of an organization. This consists relationships (assertions about associations between pairs of entity classes).</item>
          <item>a <link url="http://en.wikipedia.org/wiki/Logical_schema">logical data model</link> (schema) describes the semantics, as represented by a particular data manipulation technology. This consists of descriptions of tables and columns, object oriented classes, and XML tags, among other things.</item>
          <item>a <link url="http://en.wikipedia.org/wiki/Physical_schema">physical data model</link> (schema) describes the physical means by which data are stored. This is concerned with partitions, CPUs, tablespaces, and the like.</item>
          <item>a <link url="http://en.wikipedia.org/w/index.php?title=Data_definition&amp;action=edit">data definition</link> This is the actual coding of the database schema in the chosen development platform.</item>
          <item>a <link url="http://en.wikipedia.org/wiki/Data_manipulation">data manipulation</link> describes the operations applied to the data in the schema.</item>
        </list>
        <para id="id4711007">The significance of this approach, according to John Zachman, is that it allows the six perspectives to be relatively independent of each other. In each case, of course, the structures must remain consistent with the other model instances although the details change. The table/column structure may be different from a direct translation of the entity classes, relationships and attributes, but it must ultimately carry out the objectives of the conceptual entity class structure and contextual relationship structure. Zachman regards each instance as a separate perspective of the database not a methodology, however development projects and software tools often proceed from <link url="http://en.wikipedia.org/wiki/Conceptual_schema">conceptual data model</link>, to <link url="http://en.wikipedia.org/w/index.php?title=Contextual_data_model&amp;action=edit">contextual data model</link>, followed by the <link url="http://en.wikipedia.org/wiki/Logical_schema">logical data model</link>. In later stages when the database platform is known, this model may be translated into a <link url="http://en.wikipedia.org/wiki/Physical_schema">physical data model</link> followed by the <link url="http://en.wikipedia.org/w/index.php?title=Data_definition&amp;action=edit">data definition</link>. When the database is operational <link url="http://en.wikipedia.org/wiki/Data_manipulation">data manipulation</link> takes place.</para>
        <para id="id4506830">Different modelers may well produce different models of the same domain. This can lead to difficulty in bringing the models of different people together. Invariably, however, this difference is attributable to different levels of abstraction in the models. If the modelers agree on certain elements which are to be rendered more concretely, then the differences become less significant.</para>
        <para id="id4506841">There are generic patterns that can be used to advantage for modeling business. These include the concepts PARTY (with included PERSON and ORGANIZATION), PRODUCT TYPE, PRODUCT INSTANCE, ACTIVITY TYPE, ACTIVITY INSTANCE, CONTRACT, GEOGRAPHIC AREA, and SITE. A model which explicitly includes versions of these entity classes will be both reasonably robust and reasonably easy to understand.</para>
        <para id="id4506852">More abstract models are suitable for general purpose tools, and consist of variations on THING and THING TYPE, with all actual data being instances of these. Such abstract models are significantly more difficult to manage, since they are not very expressive of real world things. More concrete and specific data models will risk having to change as the environment changes.</para>
        <para id="id4506862">One approach to generic data modeling has the following characteristics:</para>
        <list id="id4506868" list-type="bulleted">
          <item>A generic data model shall consist of generic entity types, such as 'individual thing', 'class', 'relationship', and possibly a number of their subtypes.</item>
          <item>Every individual thing is an instance of a generic entity called 'individual thing' or one of its subtypes.</item>
          <item>Every individual thing is explicitly classified by a kind of thing ('class') using an explicit classification relationship.</item>
          <item>The classes used for that classification are separately defined as standard instances of the entity 'class' or one of its subtypes, such as 'class of relationship'. These standard classes are usually called 'reference data'. This means that domain specific knowledge is captured in those standard instances and not as entity types. For example, concepts such as car, wheel, building, ship, and also temperature, length, etc. are standard instances. But also standard types of relationship, such as 'is composed of' and 'is involved in' can be defined as standard instances.</item>
        </list>
        <para id="id4506908">This way of modeling allows the addition of standard classes and standard relation types as data (instances), which makes the data model flexible and prevents data model changes when the scope of the application changes.</para>
        <para id="id4506916">A generic data model obeys the following rules:</para>
        <list id="id4506922" list-type="enumerated">
          <item>Candidate attributes are treated as representing relationships to other entity types.</item>
          <item>Entity types are represented, and are named after, the underlying nature of a thing, not the role it plays in a particular context. Entity types are chosen.</item>
          <item>Entities have a local identifier within a database or exchange file. These should be artificial and managed to be unique. Relationships are not used as part of the local identifier.</item>
          <item>Activities, relationships and event-effects are represented by entity types (not attributes).</item>
          <item>Entity types are part of a sub-type/super-type hierarchy of entity types, in order to define a universal context for the model. As types of relationships are also entity types, they are also arranged in a sub-type/super-type hierarchy of types of relationship.</item>
          <item>Types of relationships are defined on a high (generic) level, being the highest level where the type of relationship is still valid. For example, a composition relationship (indicated by the phrase: 'is composed of') is defined as a relationship between an 'individual thing' and another 'individual thing' (and not just between e.g. an order and an order line). This generic level means that the type of relation may in principle be applied between any individual thing and any other individual thing. Additional constraints are defined in the 'reference data', being standard instances of relationships between kinds of things.</item>
        </list>
        <para id="id4506981">Examples of generic data models are ISO 10303-221, <link url="http://en.wikipedia.org/wiki/ISO_15926">ISO 15926</link> and <link url="http://en.wikipedia.org/wiki/Gellish">Gellish</link></para>
        <para id="id4507008">Data organization</para>
        <para id="id4507023">Another kind of data model describes how to organize data using a <link url="http://en.wikipedia.org/wiki/Database_management_system">database management system</link> or other data management technology. It describes, for example, relational tables and columns or object-oriented classes and attributes. Such a data model is sometimes referred to as the <link url="http://en.wikipedia.org/wiki/Physical_data_model">physical data model</link>, but in the original ANSI three schema architecture, it is called "logical". In that architecture, the physical model describes the storage media (cylinders, tracks, and tablespaces). Ideally, this model is derived from the more conceptual data model described above. It may differ, however, to account for constraints like processing capacity and usage patterns.</para>
        <para id="id4507061">While data analysis is a common term for data modeling, the activity actually has more in common with the ideas and methods of <link url="http://en.wikipedia.org/wiki/Synthesis">synthesis</link> (inferring general concepts from particular instances) than it does with <link url="http://en.wiktionary.org/wiki/Analysis">analysis</link> (identifying component concepts from more general ones). {Presumably we call ourselves <link url="http://en.wikipedia.org/w/index.php?title=Systems_analysts&amp;action=edit">systems analysts</link> because no one can say <link url="http://en.wikipedia.org/w/index.php?title=Systems_synthesists&amp;action=edit">systems synthesis</link>.} Data modeling strives to bring the data structures of interest together into a cohesive, inseparable, whole by eliminating unnecessary data redundancies and by relating data structures with <link url="http://en.wikipedia.org/wiki/Relational_model">relationships</link>.</para>
        <para id="id4507151">A different approach is through the use of <link url="http://en.wikipedia.org/wiki/Adaptive_systems">adaptive systems</link> such as <link url="http://en.wikipedia.org/wiki/Artificial_neural_networks">artificial neural networks</link> that can autonomously create implicit models of data.</para>
      </section>
      <section id="id-0261861151594">
        <title>1.3. Data structures</title>
        <para id="id3716268">(From Wikipedia, the free encyclopedia)</para>
        <para id="id3716278">In <link url="http://en.wikipedia.org/wiki/Computer_science">computer science</link>, a data structure is a way of storing <link url="http://en.wikipedia.org/wiki/Data">data</link> in a computer so that it can be used efficiently. Often a carefully chosen data structure will allow the most <link url="http://en.wikipedia.org/wiki/Algorithmic_efficiency">efficient</link><link url="http://en.wikipedia.org/wiki/Algorithm">algorithm</link> to be used. The choice of the data structure often begins from the choice of an <link url="http://en.wikipedia.org/wiki/Abstract_data_structure">abstract data structure</link>. A well-designed data structure allows a variety of critical operations to be performed, using as few resources, both execution time and memory space, as possible. Data structures are implemented using the <link url="http://en.wikipedia.org/wiki/Data_type">data types</link>, <link url="http://en.wikipedia.org/wiki/Reference_%28computer_science%29">references</link> and operations on them provided by a <link url="http://en.wikipedia.org/wiki/Programming_language">programming language</link>.</para>
        <para id="id3716390">Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to certain tasks. For example, <link url="http://en.wikipedia.org/wiki/B-tree">B-trees</link> are particularly well-suited for implementation of databases, while <link url="http://en.wikipedia.org/wiki/Routing_table">routing tables</link> rely on networks of machines to function.</para>
        <para id="id3716422">In the design of many types of programs, the choice of data structures is a primary design consideration, as experience in building large systems has shown that the difficulty of implementation and the quality and performance of the final result depends heavily on choosing the best data structure. After the data structures are chosen, the <link url="http://en.wikipedia.org/wiki/Algorithm">algorithms</link> to be used often become relatively obvious. Sometimes things work in the opposite direction - data structures are chosen because certain key tasks have algorithms that work best with particular data structures. In either case, the choice of appropriate data structures is crucial.</para>
        <para id="id3716448">This insight has given rise to many formalized design methods and <link url="http://en.wikipedia.org/wiki/Programming_language">programming languages</link> in which data structures, rather than algorithms, are the key organizing factor. Most languages feature some sort of <link url="http://en.wikipedia.org/wiki/Module_system">module system</link>, allowing data structures to be safely reused in different applications by hiding their verified implementation details behind controlled interfaces. <link url="http://en.wikipedia.org/wiki/Object-oriented">Object-oriented</link> programming languages such as <link url="http://en.wikipedia.org/wiki/C%2B%2B">C++</link> and <link url="http://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</link> in particular use <link url="http://en.wikipedia.org/wiki/Class_%28computer_science%29">classes</link> for this purpose.</para>
        <para id="id3716530">Since data structures are so crucial, many of them are included in standard libraries of modern programming languages and environments, such as C++'s <link url="http://en.wikipedia.org/wiki/Standard_Template_Library">Standard Template Library</link>, the <link url="http://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</link> Collections Framework, and the Microsoft <link url="http://en.wikipedia.org/wiki/.NET_Framework">.NET Framework</link>.</para>
        <para id="id3716574">The fundamental building blocks of most data structures are <link url="http://en.wikipedia.org/wiki/Array">arrays</link>, <link url="http://en.wikipedia.org/wiki/Record_%28computer_science%29">records</link>, <link url="http://en.wikipedia.org/wiki/Tagged_union">discriminated unions</link>, and <link url="http://en.wikipedia.org/wiki/Reference_%28computer_science%29">references</link>. For example, the nullable reference, a reference which can be null, is a combination of references and discriminated unions, and the simplest linked data structure, the <link url="http://en.wikipedia.org/wiki/Linked_list">linked list</link>, is built from records and nullable references.</para>
        <para id="id3716642">Data structures represent <link url="http://en.wikipedia.org/wiki/Implementation">implementations</link> or <link url="http://en.wikipedia.org/wiki/Interface_%28computer_science%29">interfaces</link>: A data structure can be viewed as an interface between two functions or as an implementation of methods to access storage that is organized according to the associated <link url="http://en.wikipedia.org/wiki/Datatype">data type</link>.</para>
      </section>
      <section id="id-979049919807">
        <title>1.4. Algorithms analysis</title>
        <para id="id3686092">(From Wikipedia, the free encyclopedia)</para>
        <para id="id3686102">To analyze an <link url="http://en.wikipedia.org/wiki/Algorithm">algorithm</link> is to determine the amount of resources (such as time and storage) necessary to execute it. Most algorithms are designed to work with inputs of arbitrary length. Usually the efficiency or <link url="http://en.wikipedia.org/wiki/Computational_complexity_theory">complexity</link> of an algorithm is stated as a function relating the <link url="http://en.wikipedia.org/wiki/Problem_size">input length</link> to the number of steps (time complexity) or storage locations (space complexity).</para>
        <para id="id3686168">Algorithm analysis is an important part of a broader <link url="http://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity theory</link>, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search of efficient algorithms.</para>
        <para id="id3686189">In theoretical analysis of algorithms it is common to estimate their complexity in asymptotic sense, i.e., to estimate the complexity function for reasonably large length of input. <link url="http://en.wikipedia.org/wiki/Big_O_notation">Big O notation</link>, <link url="http://en.wikipedia.org/wiki/Big_O_notation">omega</link> notation and <link url="http://en.wikipedia.org/wiki/Big_O_notation">theta</link> notation are used to this end. For instance, <link url="http://en.wikipedia.org/wiki/Binary_search">binary search</link> is said to run an amount of steps proportional to a logarithm, or in O(log(n)), colloquially "in logarithmic time". Usually asymptotic estimates are used because different <link url="http://en.wikipedia.org/wiki/Implementation">implementations</link> of the same algorithm may differ in efficiency. However the efficiencies of any two "reasonable" implementations of a given algorithm are related by a constant multiplicative factor called hidden constant.</para>
        <para id="id3686273">Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called <link url="http://en.wikipedia.org/wiki/Model_of_computation">model of computation</link>. A model of computation may be defined in terms of an <link url="http://en.wikipedia.org/wiki/Abstract_machine">abstract computer</link>, e.g., <link url="http://en.wikipedia.org/wiki/Turing_machine">Turing machine</link>, and/or by postulating that certain operations are executed in unit time. For example, if the sorted set to which we apply <link url="http://en.wikipedia.org/wiki/Binary_search">binary search</link> has N elements, and we can guarantee that a single binary lookup can be done in unit time, then at most log2 N + 1 time units are needed to return an answer.</para>
        <para id="id3686339">Exact measures of efficiency are useful to the people who actually implement and use algorithms, because they are more precise and thus enable them to know how much time they can expect to spend in execution. To some people (e.g. game programmers), a hidden constant can make all the difference between success and failure.</para>
        <para id="id3686349">Time efficiency estimates depend on what we define to be a step. For the analysis to make sense, the time required to perform a step must be guaranteed to be bounded above by a constant. One must be careful here; for instance, some analyses count an addition of two numbers as a step. This assumption may not be warranted in certain contexts. For example, if the numbers involved in a computation may be arbitrarily large, addition no longer can be assumed to require constant time (compare the time you need to add two 2-digit integers and two 1000-digit integers using a pen and paper).</para>
        <para id="id3686363"/>
      </section>
    </section>
  </content>
</document>